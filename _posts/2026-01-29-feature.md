---
layout: post
title: Feature-Aware Test Generation for Deep Learning Models
authors: Xingcheng Chen, Oliver Weißl, Andrea Stocco
link: https://arxiv.org/abs/2601.14081
categories:
- papers
- preprint
---
In this work, we examine how models behave under isolated semantic changes by directly manipulating individual features in StyleGAN’s S-space, instead of entangled Z or W representations. This enables precise, feature-level testing and helps uncover shortcut behaviors and robustness issues that are often hidden by standard accuracy-based evaluations.

For more information, click on the To the paper button at the bottom of the page.

### Abstract:

> As deep learning models are widely used in software systems, test generation plays a crucial role in assessing the quality of such models before deployment. To date, the most advanced test generators rely on generative AI to synthesize inputs; however, these approaches remain limited in providing semantic insight into the causes of misbehaviours and in offering fine-grained semantic controllability over the generated inputs. In this paper, we introduce Detect, a feature-aware test generation framework for vision-based deep learning (DL) models that systematically generates inputs by perturbing disentangled semantic attributes within the latent space. Detect perturbs individual latent features in a controlled way and observes how these changes affect the model's output. Through this process, it identifies which features lead to behavior shifts and uses a vision-language model for semantic attribution. By distinguishing between task-relevant and irrelevant features, Detect applies feature-aware perturbations targeted for both generalization and robustness. Empirical results across image classification and detection tasks show that Detect generates high-quality test cases with fine-grained control, reveals distinct shortcut behaviors across model architectures (convolutional and transformer-based), and bugs that are not captured by accuracy metrics. Specifically, Detect outperforms a state-of-the-art test generator in decision boundary discovery and a leading spurious feature localization method in identifying robustness failures. Our findings show that fully fine-tuned convolutional models are prone to overfitting on localized cues, such as co-occurring visual traits, while weakly supervised transformers tend to rely on global features, such as environmental variances. These findings highlight the value of interpretable and feature-aware testing in improving DL model reliability.